### Gaussian Process MCMC with ABC particle generation
ergmGPmcmc = function(Niter, d, N, X, stat, Domain, cycle = 100, LikEm = FALSE, num){ 
  p = 2
  # num.point = 3000           
  num.point = d*10 # LikEm2, LikEm3       
  th = matrix(0,num.point,p)
  
  # Generate D Latin hypercube design points over the domain D_1
  Design = lhsDesign(n=num.point, dimension=p, randomized=TRUE, seed=1)
  th = Design$design
  th[,1] = (Domain[2,1]-Domain[1,1])*th[,1]+Domain[1,1]
  th[,2] = (Domain[2,2]-Domain[1,2])*th[,2]+Domain[1,2]
  
  # Simulate the auxiliary variable for each design point
  summat = pAuxSamp(X, cycle, th, 1, num) # 100 cycle
  
  # Choose some of them based on the distance b/w simulated and observed summary statistics
  dist = sqrt(  apply(  ( matrix( rep(stat, num.point), num.point, byrow = T) - summat[1:num.point,,1] )^2, 1, sum ) )
  eps = quantile(dist, probs = 0.03)
  m =  apply(  th[which(dist < eps),], 2, mean)
  S = cov( th[which(dist < eps),] )
  num.point = d                     # suppose we have 'num.point' particles
  thABC = mvrnorm(num.point, m, S)
  
  # Select a smaller rectangular domain D_2 in D_1
  Domain = rbind(apply(thABC,2,min), apply(thABC,2,max))  
  th = matrix(0,num.point,p)
  
  # Generate d number of particles over the domain
  Design = lhsDesign(n=num.point, dimension=p, randomized=TRUE, seed=1)
  th = Design$design
  th[,1] = (Domain[2,1]-Domain[1,1])*th[,1]+Domain[1,1]
  th[,2] = (Domain[2,2]-Domain[1,2])*th[,2]+Domain[1,2]
  
  # Generate auxiliary variables at the sample mean of the particles
  hat = apply(thABC, 2, mean)
  Sample = pResponseErgm(X, cycle, hat, N, num) # summary statistics
  
  # Calculate importance sampling approximation to log normalizing function
  y = c()
  for(i in 1:num.point){
    cal = Sample%*%(th[i,]-hat)
    mx = max(cal)
    y[i] = mx + log( mean( exp(cal-mx) ) )
  }
  
  if(LikEm){
    lhX = c()
    for(i in 1:num.point){ lhX[i] = stat%*%th[i,] }
    y = lhX - y
  } 
  
  # Fit a Gaussian process
  m = km(~ ., design = th[1:num.point,], response = matrix(y[1:num.point],num.point,1), covtype = "matern3_2")
  
  # GLS beta coefficients
  beta.hat.gls = c(coef(m)$trend1,coef(m)$trend2,coef(m)$trend3)
  range.hat = coef(m)$range
  sd2.hat = coef(m)$sd2
  
  # initial theta and covariance matrix for proposal
  theta = matrix(hat,1)
  COV = cov(thABC)
  
  # calculating an initial value for the normalizing ftn / full likelihood
  # Kriging
  x.point = data.frame( X1 = theta[1,1], X2 = theta[1,2] )
  pred.m = predict(m, x.point, "UK")
  lhXZ = pred.m$mean
  
  if(LikEm){
    result = GPmcmcErgmLik(Niter, theta, COV, lhXZ, beta.hat.gls, c(range.hat, sd2.hat), th, y, stat)[-1,]
  } else {
    result = GPmcmcErgmNorm(Niter, theta, COV, lhXZ, beta.hat.gls, c(range.hat, sd2.hat), th, y, stat)[-1,]
  }
  
  return(result)
}


### Gaussian Process MCMC with DMH particle generation
ergmGPmcmcDMH = function(Niter, d, N, X, stat, parameter, hat, cycle = 100, LikEm = FALSE, num){ 
  th = parameter[1:d,]
  
  # Generate auxiliary variables at the sample mean of the particles
  Sample = pResponseErgm(X, cycle, hat, N, num) # summary statistics
  
  # Calculate importance sampling approximation to log normalizing function
  y = c()
  for(i in 1:d){
    cal = Sample%*%(th[i,]-hat)
    mx = max(cal)
    y[i] = mx + log( mean( exp(cal-mx) ) )
  }
  
  if(LikEm){
    lhX = c()
    for(i in 1:d){ lhX[i] = stat%*%th[i,] }
    y = lhX - y
  } 
  
  # Fit a Gaussian process
  m = km(~ ., design = th[1:d,], response = matrix(y[1:d],d,1), covtype = "matern3_2")
  
  # GLS beta coefficients
  beta.hat.gls = c(coef(m)$trend1,coef(m)$trend2,coef(m)$trend3)
  range.hat = coef(m)$range
  sd2.hat = coef(m)$sd2
  
  # initial theta and covariance matrix for proposal
  theta = matrix(hat,1)
  COV = diag(c(0.1, 0.1))
  
  # calculating an initial value for the normalizing ftn / full likelihood
  # Kriging
  x.point = data.frame( X1 = theta[1,1], X2 = theta[1,2] )
  pred.m = predict(m, x.point, "UK")
  lhXZ = pred.m$mean
  
  if(LikEm){
    result = GPmcmcErgmLik(Niter, theta, COV, lhXZ, beta.hat.gls, c(range.hat, sd2.hat), th, y, stat)[-1,]
  } else {
    result = GPmcmcErgmNorm(Niter, theta, COV, lhXZ, beta.hat.gls, c(range.hat, sd2.hat), th, y, stat)[-1,]
  }
  
  return(result)
}


### Gaussian Process MCMC with ABC particle generation
ergmABCLikPrior = function(Niter, d, N, X, stat, Domain, cycle = 100, num){ 
  p = 2
  # num.point = 3000           
  num.point = d*10 # LikEm2, LikEm3       
  th = matrix(0,num.point,p)
  
  # Generate D Latin hypercube design points over the domain D_1
  Design = lhsDesign(n=num.point, dimension=p, randomized=TRUE, seed=1)
  th = Design$design
  th[,1] = (Domain[2,1]-Domain[1,1])*th[,1]+Domain[1,1]
  th[,2] = (Domain[2,2]-Domain[1,2])*th[,2]+Domain[1,2]
  
  # Simulate the auxiliary variable for each design point
  summat = pAuxSamp(X, cycle, th, 1, num) # 100 cycle
  
  # Choose some of them based on the distance b/w simulated and observed summary statistics
  dist = sqrt(  apply(  ( matrix( rep(stat, num.point), num.point, byrow = T) - summat[1:num.point,,1] )^2, 1, sum ) )
  eps = quantile(dist, probs = 0.03)
  m =  apply(  th[which(dist < eps),], 2, mean)
  S = cov( th[which(dist < eps),] )
  num.point = d                     # suppose we have 'num.point' particles
  thABC = mvrnorm(num.point, m, S)
  
  # Select a smaller rectangular domain D_2 in D_1
  Domain = rbind(apply(thABC,2,min), apply(thABC,2,max))  
  th = matrix(0,num.point,p)
  
  # Generate d number of particles over the domain
  Design = lhsDesign(n=num.point, dimension=p, randomized=TRUE, seed=1)
  th = Design$design
  th[,1] = (Domain[2,1]-Domain[1,1])*th[,1]+Domain[1,1]
  th[,2] = (Domain[2,2]-Domain[1,2])*th[,2]+Domain[1,2]
  
  # Generate auxiliary variables at the sample mean of the particles
  hat = apply(thABC, 2, mean)
  Sample = pResponseErgm(X, cycle, hat, N, num) # summary statistics
  
  # Calculate importance sampling approximation to log normalizing function
  y = c()
  for(i in 1:num.point){
    cal = Sample%*%(th[i,]-hat)
    mx = max(cal)
    y[i] = mx + log( mean( exp(cal-mx) ) )
  }
  
  lhX = c()
  for(i in 1:num.point){ lhX[i] = stat%*%th[i,] }
  y = lhX - y
  
  # Fit a Gaussian process
  m = km(~ ., design = th[1:num.point,], response = matrix(y[1:num.point],num.point,1), covtype = "matern3_2")
  
  # GLS beta coefficients
  beta.hat.gls = c(coef(m)$trend1,coef(m)$trend2,coef(m)$trend3)
  range.hat = coef(m)$range
  sd2.hat = coef(m)$sd2
  
  # initial theta and covariance matrix for proposal
  theta = matrix(hat,1)
  COV = cov(thABC)
  
  # calculating an initial value for the normalizing ftn / full likelihood
  # Kriging
  x.point = data.frame( X1 = theta[1,1], X2 = theta[1,2] )
  pred.m = predict(m, x.point, "UK")
  lhXZ = pred.m$mean
  
  result = GPmcmcErgmLikPrior(Niter, theta, COV, lhXZ, beta.hat.gls, c(range.hat, sd2.hat), th, y, stat)[-1,]
  
  return(result)
}


### Gaussian Process MCMC with DMH particle generation
ergmDMHLikPrior = function(Niter, d, N, X, stat, parameter, hat, cycle = 100, num){ 
  th = parameter[1:d,]
  
  # Generate auxiliary variables at the sample mean of the particles
  Sample = pResponseErgm(X, cycle, hat, N, num) # summary statistics
  
  # Calculate importance sampling approximation to log normalizing function
  y = c()
  for(i in 1:d){
    cal = Sample%*%(th[i,]-hat)
    mx = max(cal)
    y[i] = mx + log( mean( exp(cal-mx) ) )
  }
  
  lhX = c()
  for(i in 1:d){ lhX[i] = stat%*%th[i,] }
  y = lhX - y 
  
  # Fit a Gaussian process
  m = km(~ ., design = th[1:d,], response = matrix(y[1:d],d,1), covtype = "matern3_2")
  
  # GLS beta coefficients
  beta.hat.gls = c(coef(m)$trend1,coef(m)$trend2,coef(m)$trend3)
  range.hat = coef(m)$range
  sd2.hat = coef(m)$sd2
  
  # initial theta and covariance matrix for proposal
  theta = matrix(hat,1)
  COV = diag(c(0.1, 0.1))
  
  # calculating an initial value for the normalizing ftn / full likelihood
  # Kriging
  x.point = data.frame( X1 = theta[1,1], X2 = theta[1,2] )
  pred.m = predict(m, x.point, "UK")
  lhXZ = pred.m$mean
  
  result = GPmcmcErgmLikPrior(Niter, theta, COV, lhXZ, beta.hat.gls, c(range.hat, sd2.hat), th, y, stat)[-1,]
  
  return(result)
}


### approximate score function
approxScore = function(Sx, Sy){
  res = Sx - colMeans(Sy)
  return(res)
}


### approximate H and J matrices
approxHJmat = function(Sx, Sy1, Sy2){
  
  Sy1bar = colMeans(Sy1)
  Sy2bar = colMeans(Sy2)
  
  h11hat = -mean(Sy1[,1]^2) + Sy1bar[1]^2
  h22hat = -mean(Sy2[,2]^2) + Sy2bar[2]^2
  h12hat = 0
  
  j11hat = Sx[1]*Sx[1] - 2*Sx[1]*Sy1bar[1] + Sy1bar[1]*Sy1bar[1]
  j12hat = Sx[1]*Sx[2] - Sx[1]*Sy2bar[2] - Sx[2]*Sy1bar[1] + Sy1bar[1]*Sy2bar[2]
  j22hat = Sx[2]*Sx[2] - 2*Sx[2]*Sy2bar[2] + Sy2bar[2]*Sy2bar[2]
  
  return(c(h11hat, h12hat, h22hat, j11hat, j12hat, j22hat))
}


### approximate score function with multiple indep chains
approxScore2 = function(Sx, Sy){
  res = Sx - c(mean(Sy[,,1]), mean(Sy[,,2]))
  return(res)
}


### approximate H and J matrices  with multiple indep chains
approxHJmat2 = function(Sx, Sy){ # Sy[,,1] is w.r.t. theta1, and Sy[,,2] is w.r.t. theta2
  m = dim(Sy)[2]
  
  Sy1bar = colMeans(Sy[,,1])
  Sy2bar = colMeans(Sy[,,2])
  
  h11hat = mean(-colMeans(Sy[,,1]^2) + Sy1bar*c(Sy1bar[2:m], Sy1bar[1]))
  h22hat = mean(-colMeans(Sy[,,2]^2) + Sy2bar*c(Sy2bar[2:m], Sy2bar[1]))
  h12hat = 0
  
  j11hat = mean(Sx[1]*Sx[1] - 2*Sx[1]*Sy1bar + Sy1bar*c(Sy1bar[2:m], Sy1bar[1]))
  j12hat = mean(Sx[1]*Sx[2] - Sx[1]*Sy2bar - Sx[2]*Sy1bar + Sy1bar*c(Sy2bar[2:m], Sy2bar[1]))
  j22hat = mean(Sx[2]*Sx[2] - 2*Sx[2]*Sy2bar + Sy2bar*c(Sy2bar[2:m], Sy2bar[1]))
  
  return(c(h11hat, h12hat, h22hat, j11hat, j12hat, j22hat))
}

### approximate H and J matrices  with multiple indep chains
approxHJmat3 = function(Sx, Sy){ # Sy[,,1] is w.r.t. theta1, and Sy[,,2] is w.r.t. theta2
  m = dim(Sy)[2]
  
  Sy1bar = colMeans(Sy[,,1])
  Sy2bar = colMeans(Sy[,,2])
  
  h11hat = mean(-colMeans(Sy[,,1]^2) + Sy1bar*c(Sy1bar[2:m], Sy1bar[1]))
  h22hat = mean(-colMeans(Sy[,,2]^2) + Sy2bar*c(Sy2bar[2:m], Sy2bar[1]))
  h12hat = mean(-colMeans(Sy[,,1]*Sy[,,2]) + Sy1bar*c(Sy2bar[2:m], Sy2bar[1]))
  
  j11hat = mean(Sx[1]*Sx[1] - 2*Sx[1]*Sy1bar + Sy1bar*c(Sy1bar[2:m], Sy1bar[1]))
  j12hat = mean(Sx[1]*Sx[2] - Sx[1]*Sy2bar - Sx[2]*Sy1bar + Sy1bar*c(Sy2bar[2:m], Sy2bar[1]))
  j22hat = mean(Sx[2]*Sx[2] - 2*Sx[2]*Sy2bar + Sy2bar*c(Sy2bar[2:m], Sy2bar[1]))
  
  return(c(h11hat, h12hat, h22hat, j11hat, j12hat, j22hat))
}


multibm = function (x, size = "sqroot", warn = FALSE){
  n = nrow(x)
  if (n < 1000) {
    if (warn) 
      warning("too few samples (less than 1,000)")
    if (n < 10) 
      return(NA)
  }
  if (size == "sqroot") {
    b = floor(sqrt(n))
    a = floor(n/b)
  }
  else if (size == "cuberoot") {
    b = floor(n^(1/3))
    a = floor(n/b)
  }
  else {
    if (!is.numeric(size) || size <= 1 || size == Inf) 
      stop("'size' must be a finite numeric quantity larger than 1.")
    b = floor(size)
    a = floor(n/b)
  }
  y = sapply(1:a, function(k) return(colMeans(x[((k - 1) * b + 1):(k * b),])))
  mu.hat = rowMeans(y)
  var.hat = b * (y - mu.hat)%*%t(y - mu.hat)/(a - 1)
  Sigma = var.hat/n
  list(est = mu.hat, Sigma = Sigma)
}


ACD = function(x, y, w = 0.5){
  xnorm = sqrt(sum(x^2))
  ynorm = sqrt(sum(y^2))
  if(all(x == y)){
    0
  } else if(xnorm == 0 | ynorm == 0){
    stop('x and y must not be the origin')
  } else{
    cosine = ifelse(xnorm == 0 | ynorm == 0, 0, sum(x*y)/(xnorm*ynorm))
    ratio = ifelse(xnorm <= ynorm, xnorm/ynorm, ynorm/xnorm)
    w*(1-cosine) + (1-w)*(1-ratio)
  }
}



# ### PRE-STEP: Particle selection =====================================
# 
# # ABC method
# partABC = function(X, stat, m, d, num){ # d: no of particles
#   # Step 1: Select a rectangle domain
#   p = 2                                          # dimension of parameter
#   wth = 5
#   # wth = 12
#   Domain = matrix(c(m$coef[1] - wth*sqrt(m$covar[1,1]), m$coef[1] + wth*sqrt(m$covar[1,1]) ,
#                     m$coef[2] - wth*sqrt(m$covar[2,2]), m$coef[2] + wth*sqrt(m$covar[2,2])),2,p)  
#   num.point = 3000           
#   th = matrix(0,num.point,p)
#   
#   # Step 2: Generate D Latin hypercube design points over the domain D_1
#   Design = lhsDesign(n=num.point, dimension=p, randomized=TRUE, seed=1)
#   th = Design$design
#   th[,1] = (Domain[2,1]-Domain[1,1])*th[,1]+Domain[1,1]
#   th[,2] = (Domain[2,2]-Domain[1,2])*th[,2]+Domain[1,2]
#   
#   # Step 3: Simulate the auxiliary variable for each design point
#   summat = pAuxSamp(X, 1, th, 1, num)
#   
#   # Step 4: Choose some of them based on the distance b/w simulated and observed summary statistics
#   dist = sqrt(  apply(  ( matrix( rep(stat, num.point), num.point, byrow = T) - summat[1:num.point,,1] )^2, 1, sum ) )
#   eps = quantile(dist, probs = 0.03)
#   m =  apply(  th[which(dist < eps),], 2, mean)
#   S = cov( th[which(dist < eps),] )
#   num.point = d                     # suppose we have 'num.point' particles
#   thABC = mvrnorm(num.point, m, S)
#   
#   # Step 5: Select a smaller rectangular domain D_2 in D_1
#   Domain = rbind(apply(thABC,2,min), apply(thABC,2,max))  
#   th = matrix(0,num.point,p)
#   
#   # Step 6: Generate d number of particles over the domain
#   Design = lhsDesign(n=num.point, dimension=p, randomized=TRUE, seed=1)
#   th = Design$design
#   th[,1] = (Domain[2,1]-Domain[1,1])*th[,1]+Domain[1,1]
#   th[,2] = (Domain[2,2]-Domain[1,2])*th[,2]+Domain[1,2]
#   
#   return(list(th, thABC))
# }
# 
# 
# ### Gaussian Process MCMC (particles generated using ABC)
# GPmcmcErgm = function(Niter, X, stat, parts, impReg, N, cycle, num, LikEm = FALSE){ # N: no of importance samples
#   num.point = nrow(parts) # no of particles
#   
#   ### (Step 1) Generate N sets of auxiliary variables for the sample mean of the particles
#   hat = apply(impReg, 2, mean)
#   Sample = pResponseErgm(X, cycle, hat, N, num)                      # summary statistics
#   
#   
#   ### (Step 2) Calculate importance sampling approximation to log normalizing function
#   # IS approximation to the normalizing constant
#   y = c()
#   for(i in 1:num.point){
#     cal = Sample%*%(parts[i,]-hat)
#     mx = max(cal)
#     y[i] = mx + log( mean( exp(cal-mx) ) )
#   }
#   
#   if(LikEm){
#     # unnormalized likelihood
#     lhX = c()
#     for(i in 1:num.point){ lhX[i] = stat%*%parts[i,] }
#     
#     # full likelihood
#     y = lhX - y
#   } 
#   
#   
#   ### (Step 3) Fit a Gaussian process
#   m = km(~ ., design = parts[1:num.point,], response = matrix(y[1:num.point],num.point,1), covtype = "matern3_2")
#   
#   # GLS beta coefficients
#   beta.hat.gls = c(coef(m)$trend1,coef(m)$trend2,coef(m)$trend3)
#   range.hat = coef(m)$range
#   sd2.hat = coef(m)$sd2
#   
#   
#   ### (Step 4) Propose
#   # initial theta and covariance matrix for proposal
#   theta = matrix(hat,1)
#   COV = cov(impReg)
#   
#   # calculating an initial value for the normalizing constant / full likelihood
#   # Kriging
#   x.point = data.frame( design = theta )
#   pred.m = predict(m, x.point, "UK")
#   lhXZ = pred.m$mean
#   
#   if(LikEm){
#     th = GPmcmcErgmLik(Niter, theta, COV, lhXZ, beta.hat.gls, c(range.hat, sd2.hat), parts, y, stat)[-1,]
#   } else {
#     th = GPmcmcErgmNorm(Niter, theta, COV, lhXZ, beta.hat.gls, c(range.hat, sd2.hat), parts, y, stat)[-1,]
#   }
#   
#   return(th)
# }